{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capitulo04.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suvKdcFqXcd5"
      },
      "source": [
        "Cap√≠tulo 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLiVqPvwmCJB"
      },
      "source": [
        "import gym\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "#ENV_NAME = \"FrozenLake8x8-v0\"  \n",
        "GAMMA = 0.95"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxTWfToPrjxe"
      },
      "source": [
        "### The Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgnZfMGvk9Rl"
      },
      "source": [
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.V = np.zeros(self.env.observation_space.n)\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        action_value = sum([prob*(r + GAMMA * self.V[s_])\n",
        "                          for prob, s_, r, _ in self.env.P[state][action]]) \n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                           for action in range(self.env.action_space.n)\n",
        "                           ]\n",
        "            self.V[state] = max(state_values)\n",
        "        return self.V"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scTw9juUrf3u"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOo7rC6FkyMC"
      },
      "source": [
        "TEST_EPISODES = 40\n",
        "\n",
        "def check_improvements():\n",
        "        test_env = gym.make(ENV_NAME)\n",
        "        reward_test = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            total_reward = 0.0\n",
        "            state = test_env.reset()\n",
        "            while True:\n",
        "                action = agent.select_action(state)\n",
        "                new_state, new_reward, is_done, _ = test_env.step(action)\n",
        "                total_reward += new_reward\n",
        "                if is_done: break\n",
        "                state = new_state\n",
        "            reward_test += total_reward\n",
        "        reward_test /= TEST_EPISODES\n",
        "        return (reward_test)   \n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZz8FUkByRTS"
      },
      "source": [
        "REWARD_THRESHOLD = 0.90\n",
        "\n",
        "def train(agent): \n",
        "  writer = SummaryWriter()\n",
        "\n",
        "  t = 0\n",
        "  best_reward = 0.0\n",
        " \n",
        "  while best_reward < REWARD_THRESHOLD:\n",
        "\n",
        "        agent.value_iteration()\n",
        "\n",
        "        t += 1\n",
        "        reward_test = check_improvements()\n",
        "\n",
        "        writer.add_scalar(\"reward\", reward_test, t)\n",
        "        \n",
        "        if reward_test > best_reward:\n",
        "            print(\"Best reward updated %.2f at iteration %d \" % (reward_test ,t) )\n",
        "            best_reward = reward_test\n",
        "\n",
        "  writer.close()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwNSh4R0lshO",
        "outputId": "c441bee2-d049-4459-948a-aa47dd8da42e"
      },
      "source": [
        "agent = Agent()\n",
        "train(agent)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.17 at iteration 3 \n",
            "Best reward updated 0.35 at iteration 4 \n",
            "Best reward updated 0.38 at iteration 7 \n",
            "Best reward updated 0.53 at iteration 8 \n",
            "Best reward updated 0.75 at iteration 13 \n",
            "Best reward updated 0.80 at iteration 16 \n",
            "Best reward updated 0.88 at iteration 24 \n",
            "Best reward updated 0.90 at iteration 122 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phD7ZXEU1JEL"
      },
      "source": [
        "def extract_policy(agent):   \n",
        "    env = gym.make(ENV_NAME)\n",
        "    policy = np.zeros(env.observation_space.n) \n",
        "    for s in range(env.observation_space.n):\n",
        "        Q_values = [agent.calc_action_value(s,a) for a in range(env.action_space.n)] \n",
        "        policy[s] = np.argmax(np.array(Q_values))        \n",
        "    return policy\n",
        "\n",
        "def print_policy(policy):\n",
        "    print(policy.reshape([-1, 4]))\n",
        "    print(\"\\n\")\n",
        "    a2w = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
        "    policy_arrows = [a2w[x] for x in policy]\n",
        "    print(np.array(policy_arrows).reshape([-1, 4]))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE-evSt6tnGi",
        "outputId": "ab332e39-d689-4ead-a897-98bf77cc3dc3"
      },
      "source": [
        "policy=extract_policy(agent)\n",
        "print_policy(policy)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "\n",
            "\n",
            "[['<' '^' '<' '^']\n",
            " ['<' '<' '<' '<']\n",
            " ['^' 'v' '<' '<']\n",
            " ['<' '>' 'v' '<']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4avOvWPjXigX",
        "outputId": "1006b124-8403-444f-f072-88b34ccfde97"
      },
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "env.env.P[4][0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False),\n",
              " (0.3333333333333333, 8, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipzqA0qpclTy",
        "outputId": "ff0b2304-f336-460b-953f-e839653cd37e"
      },
      "source": [
        "env.env.P[8][3]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 9, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False),\n",
              " (0.3333333333333333, 8, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHUCg19lBafh"
      },
      "source": [
        "### Test the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPjUCudxM8S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c5d43e-fc2c-42bb-9b80-2d48d06080e3"
      },
      "source": [
        "def test(agent):\n",
        "    new_test_env = gym.make(ENV_NAME) \n",
        "    state = new_test_env.reset()\n",
        "    new_test_env.render()\n",
        "    is_done = False\n",
        "    t = 0\n",
        "\n",
        "    while not is_done:\n",
        "        action = agent.select_action(state)\n",
        "        new_state, reward, is_done, _ = new_test_env.step(action)\n",
        "        new_test_env.render()\n",
        "        state = new_state\n",
        "        t +=1\n",
        "    print(\"\\nlast state =\", state)\n",
        "    print(\"reward =    \", reward)\n",
        "    print(\"time steps =\", t)\n",
        "\n",
        "test(agent)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "last state = 5\n",
            "reward =     0.0\n",
            "time steps = 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymICwXwAkyuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531aa908-239e-495f-f6ea-92afa2134f11"
      },
      "source": [
        "%load_ext tensorboard\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmobnomGlHBt"
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl0BbyRJ-M_x"
      },
      "source": [
        "### The Agent that estimates the Transition Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1eGAk3_-KTp"
      },
      "source": [
        "N =100\n",
        "import collections\n",
        "\n",
        "class Agent_Updated:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(\n",
        "            collections.Counter)\n",
        "        self.V = np.zeros(self.env.observation_space.n)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            if is_done:\n",
        "                self.state = self.env.reset() \n",
        "            else: \n",
        "                self.state = new_state\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for s_, count in target_counts.items():\n",
        "            r = self.rewards[(state, action, s_)]\n",
        "            prob = (count / total)\n",
        "            action_value += prob*(r + GAMMA * self.V[s_])\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "\n",
        "    def value_iteration(self):\n",
        "        self.play_n_random_steps(N)\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [\n",
        "                self.calc_action_value(state, action)\n",
        "                for action in range(self.env.action_space.n)\n",
        "            ]\n",
        "            self.V[state] = max(state_values)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-s8R6r-CG_2",
        "outputId": "d7f5db9c-51dc-465d-c524-d5c2591dd707"
      },
      "source": [
        "agent = Agent_Updated()\n",
        "train(agent)\n",
        "policy=extract_policy(agent)\n",
        "print_policy(policy)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.17 at iteration 13 \n",
            "Best reward updated 0.42 at iteration 19 \n",
            "Best reward updated 0.45 at iteration 21 \n",
            "Best reward updated 0.50 at iteration 22 \n",
            "Best reward updated 0.53 at iteration 28 \n",
            "Best reward updated 0.65 at iteration 30 \n",
            "Best reward updated 0.68 at iteration 31 \n",
            "Best reward updated 0.75 at iteration 40 \n",
            "Best reward updated 0.85 at iteration 112 \n",
            "Best reward updated 0.88 at iteration 169 \n",
            "Best reward updated 0.90 at iteration 562 \n",
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "\n",
            "\n",
            "[['<' '^' '<' '^']\n",
            " ['<' '<' '<' '<']\n",
            " ['^' 'v' '<' '<']\n",
            " ['<' '>' 'v' '<']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-djx4eMWDWWR"
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}