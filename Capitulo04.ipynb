{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capitulo04.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suvKdcFqXcd5"
      },
      "source": [
        "# Cap√≠tulo 04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLiVqPvwmCJB"
      },
      "source": [
        "import gym\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "#ENV_NAME = \"FrozenLake8x8-v0\"  \n",
        "GAMMA = 0.95"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxTWfToPrjxe"
      },
      "source": [
        "### The Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgnZfMGvk9Rl"
      },
      "source": [
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.V = np.zeros(self.env.observation_space.n)\n",
        "        \n",
        "    def calc_action_value(self, state, action):\n",
        "        action_value = sum([prob*(reward + GAMMA*self.V[next_state])\n",
        "                            for prob, next_state, reward, _ \n",
        "                            in self.env.P[state][action]]) \n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action = best_value = None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if not best_value or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = []\n",
        "            for action in range(self.env.action_space.n):  \n",
        "                state_values.append(self.calc_action_value(state, action))\n",
        "            self.V[state] = max(state_values)\n",
        "        return self.V\n",
        "        \n",
        "    def value_iteration_v2(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                            for action in range(self.env.action_space.n)]\n",
        "            self.V[state] = max(state_values)\n",
        "        return self.V\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scTw9juUrf3u"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOo7rC6FkyMC"
      },
      "source": [
        "TEST_EPISODES = 40\n",
        "\n",
        "def check_improvements():\n",
        "    test_env = gym.make(ENV_NAME)\n",
        "    reward_test = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        total_reward = 0.0\n",
        "        state = test_env.reset()\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            new_state, new_reward, is_done, _ = test_env.step(action)\n",
        "            total_reward += new_reward\n",
        "            if is_done: \n",
        "                break\n",
        "            state = new_state\n",
        "        reward_test += total_reward\n",
        "    reward_test /= TEST_EPISODES\n",
        "    return reward_test   "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZz8FUkByRTS"
      },
      "source": [
        "REWARD_THRESHOLD = 0.9\n",
        "\n",
        "def train(agent): \n",
        "    writer = SummaryWriter()\n",
        "    t = 0\n",
        "    best_reward = 0.0\n",
        " \n",
        "    while best_reward < REWARD_THRESHOLD:\n",
        "        agent.value_iteration()\n",
        "        t += 1\n",
        "        reward_test = check_improvements()\n",
        "        writer.add_scalar(\"reward\", reward_test, t)\n",
        "               \n",
        "        if reward_test > best_reward:\n",
        "            print(f\"Best reward updated {reward_test:.2f} at iteration {t}\") \n",
        "            best_reward = reward_test\n",
        "    writer.close()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwNSh4R0lshO",
        "outputId": "c9670828-8ea1-40e1-df74-e81cec67a2a6"
      },
      "source": [
        "agent = Agent()\n",
        "train(agent)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.35 at iteration 3\n",
            "Best reward updated 0.38 at iteration 5\n",
            "Best reward updated 0.42 at iteration 6\n",
            "Best reward updated 0.47 at iteration 7\n",
            "Best reward updated 0.50 at iteration 10\n",
            "Best reward updated 0.75 at iteration 13\n",
            "Best reward updated 0.80 at iteration 14\n",
            "Best reward updated 0.85 at iteration 18\n",
            "Best reward updated 0.93 at iteration 42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phD7ZXEU1JEL"
      },
      "source": [
        "def extract_policy(agent):   \n",
        "    env = gym.make(ENV_NAME)\n",
        "    policy = np.zeros(env.observation_space.n) \n",
        "    for s in range(env.observation_space.n):\n",
        "        Q_values = [agent.calc_action_value(s,a) for a in range(env.action_space.n)] \n",
        "        policy[s] = np.argmax(np.array(Q_values))        \n",
        "    return policy\n",
        "\n",
        "def print_policy(policy):\n",
        "    print(policy.reshape([-1, 4]))\n",
        "    print(\"\\n\")\n",
        "    visual_help = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
        "    policy_arrows = [visual_help[x] for x in policy]\n",
        "    print(np.array(policy_arrows).reshape([-1, 4]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE-evSt6tnGi",
        "outputId": "5e154056-9c38-42c3-d772-401470e77a02"
      },
      "source": [
        "policy=extract_policy(agent)\n",
        "print_policy(policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "\n",
            "\n",
            "[['<' '^' '<' '^']\n",
            " ['<' '<' '<' '<']\n",
            " ['^' 'v' '<' '<']\n",
            " ['<' '>' 'v' '<']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4avOvWPjXigX",
        "outputId": "9c1f728b-03b7-4cb0-f318-474870b8970a"
      },
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "env.env.P[4][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False),\n",
              " (0.3333333333333333, 8, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipzqA0qpclTy",
        "outputId": "a4ac0e5b-16f5-4084-e800-8fea7fce21a2"
      },
      "source": [
        "env.env.P[8][3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 9, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False),\n",
              " (0.3333333333333333, 8, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHUCg19lBafh"
      },
      "source": [
        "### Test the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPjUCudxM8S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4432147f-dd99-4bff-f133-8dd0b0b3ce99"
      },
      "source": [
        "def test(agent):\n",
        "    new_test_env = gym.make(ENV_NAME) \n",
        "    state = new_test_env.reset()\n",
        "    new_test_env.render()\n",
        "    is_done = False\n",
        "    t = 0\n",
        "\n",
        "    while not is_done:\n",
        "        action = agent.select_action(state)\n",
        "        new_state, reward, is_done, _ = new_test_env.step(action)\n",
        "        new_test_env.render()\n",
        "        state = new_state\n",
        "        t += 1\n",
        "    print(\"\\nlast state =\", state)\n",
        "    print(\"reward =    \", reward)\n",
        "    print(\"time steps =\", t)\n",
        "\n",
        "test(agent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "last state = 15\n",
            "reward =     1.0\n",
            "time steps = 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymICwXwAkyuC"
      },
      "source": [
        "%load_ext tensorboard\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmobnomGlHBt"
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl0BbyRJ-M_x"
      },
      "source": [
        "### The Agent that estimates the Transition Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1eGAk3_-KTp"
      },
      "source": [
        "import collections\n",
        "\n",
        "N = 100\n",
        "\n",
        "class AgentUpdated:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.V = np.zeros(self.env.observation_space.n)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            if is_done:\n",
        "                self.state = self.env.reset() \n",
        "            else: \n",
        "                self.state = new_state\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for s_, count in target_counts.items():\n",
        "            r = self.rewards[(state, action, s_)]\n",
        "            prob = (count / total)\n",
        "            action_value += prob*(r + GAMMA*self.V[s_])\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "\n",
        "    def value_iteration(self):\n",
        "        self.play_n_random_steps(N)\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [\n",
        "                self.calc_action_value(state, action)\n",
        "                for action in range(self.env.action_space.n)\n",
        "            ]\n",
        "            self.V[state] = max(state_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-s8R6r-CG_2",
        "outputId": "5ab72c1c-2be1-4a01-c59b-3d2a4a7d2d28"
      },
      "source": [
        "agent = AgentUpdated()\n",
        "train(agent)\n",
        "policy=extract_policy(agent)\n",
        "print_policy(policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.10 at iteration 26\n",
            "Best reward updated 0.17 at iteration 29\n",
            "Best reward updated 0.38 at iteration 32\n",
            "Best reward updated 0.47 at iteration 38\n",
            "Best reward updated 0.50 at iteration 41\n",
            "Best reward updated 0.53 at iteration 46\n",
            "Best reward updated 0.68 at iteration 53\n",
            "Best reward updated 0.78 at iteration 55\n",
            "Best reward updated 0.80 at iteration 108\n",
            "Best reward updated 0.82 at iteration 239\n",
            "Best reward updated 0.85 at iteration 265\n",
            "Best reward updated 0.88 at iteration 745\n",
            "Best reward updated 0.90 at iteration 920\n",
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "\n",
            "\n",
            "[['<' '^' '<' '^']\n",
            " ['<' '<' '<' '<']\n",
            " ['^' 'v' '<' '<']\n",
            " ['<' '>' 'v' '<']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-djx4eMWDWWR"
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}